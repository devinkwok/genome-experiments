
- see https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch
    - turn off training related layers during evaluation
    - dropout turns off automatically in this way


Data input issues
-----------
 - the data pipeline works but is poorly designed
 - it converts to 1-hot first and then applies data augmentation and batching
 - instead, it should convert to numerical indexes first and then apply 1-hot only for input to network
 - similarly, the network decoding step has to convert indexes (via argmax) back to one-hot for comparison
 - this was to support possibility of predicting empty (all zeros), but subsequent discussion has shown this is not necessary (we always want to predict a base)
 - these problems won't be fixed for the time being, as this autoencoder is a prototype
 - future models should treat inputs as indexed tokens and consider empty as a unique token, separate from masked token and possibly other semantic distinctions


Testing
-------
 - need to include dedicated testing code
 - use entire testing set to reduce stochastic nature of scores
 - be able to toggle model.eval() on for true accuracy, vs off for testing with dropout and noise
 - show examples of model reconstructions


Implementation notes
-----------
 - consider hyperparameters that can be modified during training (make hyperparams an object which is called to get a specific hyperparam, can take some kind of reference to calling object to track epochs? or find library solution)
 - SummaryWriter collects training stats for visualization via TensorBoard
 - don't shuffle validation data, also double validation batch size to take advantage of lack of gradients in memory
 - see https://pytorch.org/tutorials/beginner/nn_tutorial.html for a basic rundown of the pytorch library
 - can't use BCEWithLogitsLoss because we are using Softmax, not Sigmoid, ignore the numerical instability for the prototype, but may have to manually implement loss function to combine log operations in future
 - looks like neighbour loss isn't very useful, needs more testing


Multilayer issues
-----------------
 - not enough memory to load large sequence, this is due to data handling issues above
 - try a dilation-based instead of pooling based architecture
 - also try avgpool instead of maxpool

Visualization
------------
 - different brightness for overlapping genes?
 - pretty sure region annotation visualization is bugged

- save checkpoints for model, maybe add tensorboard

HPC
 - tensors are immutable and return a new tensor when sent to another device using `to()`, whereas models are objects and sent to device simply by calling `to()`, that is:
 `cuda_tensor = cpu_tensor.to(cuda_device)  # cpu_tensor is still on cpu, cuda_tensor is on device`
 `model.to(cuda_device)  # don't need to reassign`
 - data parallelization: include loss function in forward pass https://blog.paperspace.com/pytorch-memory-multi-gpu-debugging/
 - `del` tensors as soon as it is used to avoid memory leak (or modify in place?)
 - gradients are kept until `backward` is called, this can cause memory leak for tensors with gradients, solution is to calculate using `tensor.items()`
 - multiprocess may need to run `torch.cuda.empy_cache()`
 - use cudnn library if available:
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.enabled = True

- order matters for input, target in BCELoss, note that the order is (input, target) where the formula is (target * log(input) + (1-target) * log(1-input))
- download p13 build for GRCh38 data
- there is information leakage with current SequenceDataset since train/test sequences can overlap

Diagnostic
    use k-mers to plot errors
        build k-mers from indicated points only (errors)
    - create separate test set
    - include test and validation error rates
    - verify on randomized data (permutations at different window lengths)
    optional
        - normalize and visualize latent vectors
        - try different omissions of data (with and without empty 'N' sequences, omitting certain k-mers)
    optional k-mers issues
        - fix running out of memory
            - root to leaf references only
            - instead of storing each character, make character implicit in array of pointers
        - 2D plot of regional k-mer frequencies as a function of window size and position
        - verify k-mer probabilities are multiplicative: e.g. for tri-mer 'ABC', P('ABC') = P('AB')P('C') = P('A')P('BC')
        - calculate k-mer frequencies relative to frequency of smaller k-mers (i.e. LD)

k-fold cross validation

loss
    change to binary cross entropy loss
    include vector to vector loss
        - include distance for similar sequences in loss function (e.g. shifting window by one bp)? (need non-significant sequence variants)
        - plot this distance across the chromosome positions

multilayer
    build architecture out to 500 bp window (use dilated convolutions?)

inference
    test imputation accuracy on fixed genome and variants
    identify variant positions
        LD
            - use vcftools, samtools to get counts of variants from vcf files
            - plot variant positions in chr22, along with probabilities and number
            - 2D plot variant LD as a function of genome distance and LD strength
        turn variants into a tensor input
        build a supervised model taking latent var as input to predict variant position
    identify TF binding

variational
    make into VAE
        - output needs to include mean and variance tensors
        - add reparameterization layer which samples normal according to mean and variance
        - add loss which includes KL divergence of intermediary layers

deploy
    run on GPU
    run on cluster


see https://github.com/ShayanPersonal/stacked-autoencoder-pytorch/blob/master/model.py
- have the input window size vary, see what conditions allow autoencoder to have zero reconstruction error
- use missing inputs (input noise) to replicate Shannon English bits per word study
- try momentum, try varying learn rate

- SIFT/PolyPhen data can be used to find functional variants
- could use Selene and train directly with DeepSEA data

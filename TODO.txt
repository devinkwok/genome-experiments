General
 - get git repo set up with commits

Visualization
    LD
        - use vcftools, samtools to get counts of variants from vcf files
        - plot variant positions in chr22, along with probabilities and number
        - 2D plot variant LD as a function of genome distance and LD strength
    k-mers
        - fix running out of memory due to recursive trie structure (switch to preallocated or hash)
        - 2D plot of regional k-mer frequencies as a function of window size and position
        - verify k-mer probabilities are multiplicative: e.g. for tri-mer 'ABC', P('ABC') = P('AB')P('C') = P('A')P('BC')
        - calculate k-mer frequencies relative to frequency of smaller k-mers (i.e. LD)
        - try to replicate results from kmermaid BoG talk (k-mers )

Autoencoder
    see https://github.com/ShayanPersonal/stacked-autoencoder-pytorch/blob/master/model.py
    - build a simple autoencoder with noise (dropout) to test imputation of missing values
        - add noise/dropout to inputs and intermediate layers (imputation)
        - include distance for similar sequences in loss function (e.g. shifting window by one bp)? (need non-significant sequence variants)
        - verify encoder by training on chr22 and testing on randomly generated sequences, and vice versa (reconstruction error should be higher when cross domain)
    - have the input window size vary, see what conditions allow autoencoder to have zero reconstruction error
    - use missing inputs (input noise) to replicate Shannon English bits per word study


    - include test and validation error rates
    - verify on randomized data (permutations at different window lengths)
    - normalize and visualize latent vectors
    - try different omissions of data (with and without empty 'N' sequences, omitting certain k-mers)

    - measure error rates on k-mers

    - need to understand deconvolution better, i.e. how is overlap handled?

    - see https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch
        - turn off training related layers during evaluation
        - dropout turns off automatically in this way

    - cutoff for empty prediction is an important hyperparameter